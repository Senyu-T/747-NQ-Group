{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NQ_bertbilstm_10k_step.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGUkMDujkUtz"
      },
      "source": [
        "!git clone https://github.com/Senyu-T/unifiedqa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq2WmVfh3f1N"
      },
      "source": [
        "Fetch Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2kwLxnzEzce"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNlgBI9fGn4X"
      },
      "source": [
        "cd unifiedqa/bart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeGQOSHLk9Pw"
      },
      "source": [
        "!chmod +x download_data.sh; ./download_data.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-WIxDi3eFfM"
      },
      "source": [
        "cd data/natural_questions_with_dpr_para/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuzI8Be_dOQF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LPKS4CYFz9q"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/NQ\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW3AEY24FOXe"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def normalize_answer(s):\n",
        "  def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def remove_spc_token(s):\n",
        "  s = s.replace(' \\\\\\'\\\\\\'', ' \\'\\'')   # double quotation\n",
        "  s = s.replace('\\\\\\'', '\\'')\n",
        "  s = s.replace(' \\'s', '\\'s')    # 's\n",
        "  s = s.replace(' ,', ',')\n",
        "  return s"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U50Luz_FQU2"
      },
      "source": [
        "# read file, parse into context / question / answer for further data analysis\n",
        "def read_files(file_name):\n",
        "  answers = []\n",
        "  questions = []\n",
        "  contexts = []\n",
        "  with open(file_name, 'rb') as inference_in:\n",
        "    lines = inference_in.readlines()\n",
        "    for i in range(len(lines)):\n",
        "      sep = str(lines[i]).split('\\\\n') \n",
        "      questions.append(sep[0][2:-1])\n",
        "      ans = (sep[1].split('\\\\t')[-1]).lower()\n",
        "      ans = normalize_answer(remove_spc_token(ans))  # normalize answers\n",
        "      answers.append(ans)\n",
        "      contexts.append(sep[1].split('\\\\t')[0])\n",
        "  return answers, questions, contexts"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf_SfRWt3iZt"
      },
      "source": [
        "Get answers, question, contexts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9NjH_9DGToh"
      },
      "source": [
        "tr_answers, tr_questions, tr_contexts = read_files(\"/content/unifiedqa/bart/data/natural_questions_with_dpr_para/train.tsv\")\n",
        "val_answers, val_questions, val_contexts = read_files(\"/content/unifiedqa/bart/data/natural_questions_with_dpr_para/dev.tsv\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8djdfdG3mlq"
      },
      "source": [
        "Save data as ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1gStXvb3oC0"
      },
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/NQ/data/answer_tags/gold_l2i.json\") as f:\n",
        "  label2index = json.load(f)\n",
        "with open(\"/content/drive/MyDrive/NQ/data/answer_tags/gold_i2l.json\") as f:\n",
        "  index2label = json.load(f)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfe1wVJm4xb9",
        "outputId": "e5976a77-f9df-4903-ab8b-307163bd4237"
      },
      "source": [
        "print(index2label)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ORG': 0, 'PERSON': 1, 'DATE': 2, 'NORP': 3, 'OTHERS': 4, 'LAW': 5, 'FAC': 6, 'PERCENT': 7, 'WORK_OF_ART': 8, 'CARDINAL': 9, 'TIME': 10, 'ORDINAL': 11, 'EVENT': 12, 'LANGUAGE': 13, 'MONEY': 14, 'GPE': 15, 'LOC': 16, 'QUANTITY': 17, 'PRODUCT': 18}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVRF-2IS4y_P",
        "outputId": "51c82d8c-51cc-4922-f0c5-39db1b38204b"
      },
      "source": [
        "print(label2index)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0': 'ORG', '1': 'PERSON', '2': 'DATE', '3': 'NORP', '4': 'OTHERS', '5': 'LAW', '6': 'FAC', '7': 'PERCENT', '8': 'WORK_OF_ART', '9': 'CARDINAL', '10': 'TIME', '11': 'ORDINAL', '12': 'EVENT', '13': 'LANGUAGE', '14': 'MONEY', '15': 'GPE', '16': 'LOC', '17': 'QUANTITY', '18': 'PRODUCT'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkjFRWcSc_xC"
      },
      "source": [
        "def get_labels(tsv_file):\n",
        "  index2label, label2index = None, None\n",
        "  tsv_file = open(tsv_file)\n",
        "  read_tsv = csv.reader(tsv_file)\n",
        "  labels = [row[0] for row in read_tsv]\n",
        "  return labels"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y81GrgqNebhm",
        "outputId": "98a4c8d3-d7db-4995-9050-783fadcf5ad0"
      },
      "source": [
        "tr_tags = \"/content/drive/MyDrive/NQ/data/answer_tags/train_soft_tag.tsv\"\n",
        "tr_index = get_labels(tr_tags)\n",
        "print(tr_index[:10])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CARDINAL', 'CARDINAL', 'PERSON', 'LOC', 'PERSON', 'OTHERS', 'PERSON', 'OTHERS', 'DATE', 'CARDINAL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxtRrFiQ5Mz3"
      },
      "source": [
        "tr_labels = [index2label[tr_index[i]] for i in range(len(tr_index))]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7qHXERh5mto"
      },
      "source": [
        "import numpy as np\n",
        "tr_labels = np.array(tr_labels)\n",
        "with open(\"/content/drive/MyDrive/NQ/data/answer_tags/tr_labels.npy\", 'wb') as f:\n",
        "  np.save(f, tr_labels)\n"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4KyCx7p5Dpa"
      },
      "source": [
        "val_tags = \"/content/drive/MyDrive/NQ/data/answer_tags/dev_soft_tag.tsv\"\n",
        "val_index = get_labels(val_tags)\n",
        "val_labels = np.array([index2label[val_index[i]] for i in range(len(val_index))])\n",
        "with open(\"/content/drive/MyDrive/NQ/data/answer_tags/val_labels.npy\", 'wb') as f:\n",
        "  np.save(f, val_labels)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYaPnggI7BJd"
      },
      "source": [
        "Load tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKhsnLPY6iJe",
        "outputId": "352c7af9-8880-4b13-87e0-9e001661220a"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/NQ/data/answer_tags/tr_labels.npy\", \"rb\") as f:\n",
        "  tr_labels = np.load(f)\n",
        "print(tr_labels[:10])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 9  9  1 16  1  4  1  4  2  9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaU_hYG67Cm_",
        "outputId": "bb1f0c5b-4386-405a-dda9-8afe411f491f"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/NQ/data/answer_tags/val_labels.npy\", \"rb\") as f:\n",
        "  val_labels = np.load(f)\n",
        "print(val_labels[:10])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  9 16  2  9  1  1  2  1  4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEeejJNDGnON"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWl1SsGxHs2H"
      },
      "source": [
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, BertTokenizerFast, BertTokenizer, AutoTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwbIIL6vGDM9"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self,embedding_dim, hidden_dim, num_layers, num_classes):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                    hidden_size=hidden_dim,\n",
        "                    num_layers=num_layers,\n",
        "                    batch_first=False,\n",
        "                    bidirectional=True)\n",
        "        self.fc1 = torch.nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, num_classes)\n",
        "    \n",
        "    \n",
        "    def forward(self, embeddings):\n",
        "        self.lstm.flatten_parameters()\n",
        "        lstm_output, _ = self.lstm(embeddings)\n",
        "        output = lstm_output[:,-1,:]\n",
        "        output = self.fc1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        return output\n",
        "\n",
        "class BertBiLSTM(nn.Module):\n",
        "    def __init__(self, num_layers, num_classes, embedding_dim = 768, hidden_dim=128, freeze=False):\n",
        "        super(BertBiLSTM, self).__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        if freeze:\n",
        "          for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.classifier = BiLSTM(self.embedding_dim, hidden_dim, num_layers, num_classes)\n",
        "       \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        text_embeddings = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_embeddings = text_embeddings[0]\n",
        "        \n",
        "        output = self.classifier(text_embeddings)\n",
        "        return output"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OWAS6IXGEib"
      },
      "source": [
        "MAX_LEN = 64\n",
        "\n",
        "def preprocessing_for_bert(tokenizer, sentences):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    for i, sent in enumerate(sentences):\n",
        "      encoded_sent = tokenizer.encode_plus(text=sent, \n",
        "                          add_special_tokens=True,        \n",
        "                          max_length=MAX_LEN,               \n",
        "                          padding='max_length',         \n",
        "                          return_attention_mask=True, \n",
        "                          truncation=True)     \n",
        "      input_ids.append(encoded_sent.get('input_ids'))\n",
        "      attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    return input_ids, attention_masks\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxhuePqEGGGH"
      },
      "source": [
        "def get_data(file_path, npy_file, tokenizer, save_location, label2index=None, index2label=None):\n",
        "    answers, questions, contexts = read_files(file_path)\n",
        "    with open(npy_file, 'rb') as f:\n",
        "      labels = np.load(f)\n",
        "\n",
        "    input_ids, attention_masks = preprocessing_for_bert(tokenizer, questions)\n",
        "    np.savez(save_location, input_ids=input_ids, attention_masks=attention_masks, labels=labels)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhYoOeCWGId_"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeoKRsbbIVI8"
      },
      "source": [
        "train_path = \"/content/drive/MyDrive/NQ/data/raw_data/train.tsv\"\n",
        "val_path = \"/content/drive/MyDrive/NQ/data/raw_data/dev.tsv\"\n",
        "tr_tags = \"/content/drive/MyDrive/NQ/data/answer_tags/tr_labels.npy\"\n",
        "val_tags = \"/content/drive/MyDrive/NQ/data/answer_tags/val_labels.npy\""
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmNG1t7IJHly"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/NQ\")"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc-ew8t4JKr8"
      },
      "source": [
        "get_data(train_path, tr_tags, tokenizer, \"data/tr_tokenized.npz\")"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsyVye7NJctJ"
      },
      "source": [
        "get_data(val_path, val_tags, tokenizer, \"data/val_tokenized.npz\")"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tM1RG4CKCHu"
      },
      "source": [
        "def load_dataset(location):\n",
        "    data = dict(np.load(location,allow_pickle=True))\n",
        "    for key, elem in data.items():\n",
        "      data[key] = torch.tensor(elem)\n",
        "    dataset = TensorDataset(data['input_ids'].squeeze(), data['attention_masks'].squeeze(), data['labels'])\n",
        "    return dataset "
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2JVDEP0KFhn"
      },
      "source": [
        "train_path = \"data/tr_tokenized.npz\"\n",
        "train_dataset = load_dataset(train_path)\n",
        "val_path = \"data/val_tokenized.npz\"\n",
        "val_dataset = load_dataset(val_path)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbfJmQFPoc5e"
      },
      "source": [
        "Main BERT code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-boZpZ-2Kbpv"
      },
      "source": [
        "def evaluate(network, loader, loss_fn, data_size):\n",
        "    loss = 0.0\n",
        "    acc = 0.0\n",
        "    with torch.no_grad():\n",
        "      for i, (input_ids, masks, labels) in tqdm(enumerate(loader),total=len(loader),position=0, leave=True):\n",
        "          input_ids = input_ids.cuda()\n",
        "          masks = masks.cuda()\n",
        "          labels = labels.cuda()\n",
        "            \n",
        "          output = network(input_ids, masks)\n",
        "          loss += loss_fn(output,labels)\n",
        "          preds = torch.argmax(output,dim=1)\n",
        "          #output = network(input_ids, masks, labels=labels)\n",
        "          #preds = torch.argmax(output.logits, dim=1)\n",
        "          #loss = output.loss\n",
        "\n",
        "          acc += torch.eq(preds, labels).sum().item()\n",
        "    return loss.item() / data_size, acc / data_size"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oqJBoupKc8Q"
      },
      "source": [
        "def train(directory, network, loss_fn, train_dataset, test_dataset, optimizer, scheduler, batch_size, num_epochs, verbose=True, val_freq=2):\n",
        "    train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=batch_size)\n",
        "    val_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "    \n",
        "    train_loss, train_acc = torch.zeros(num_epochs), torch.zeros(num_epochs)\n",
        "    test_loss, test_acc = torch.zeros(num_epochs//val_freq + 1), torch.zeros(num_epochs//val_freq + 1)\n",
        "    val_best_acc, train_best_acc = 0.0, 0.0\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    for epoch in range(num_epochs):\n",
        "        if epoch % val_freq == 0:\n",
        "          network.eval()\n",
        "          idx = epoch//val_freq\n",
        "          test_loss[idx], test_acc[idx] = evaluate(network, val_dataloader, loss_fn, len(test_dataset))\n",
        "          if test_acc[idx] > val_best_acc:\n",
        "            val_best_acc = test_acc[idx]\n",
        "            torch.save(network.state_dict(), f\"{directory}/snapshot_val_best\")\n",
        "          if verbose:\n",
        "            print(f\"epoch:{epoch:3d}, test_loss: {test_loss[idx]:3.6f}, test_acc: {test_acc[idx]:3.5f}\")\n",
        "\n",
        "        network.train()\n",
        "        train_epoch_loss, train_epoch_acc = 0.0, 0.0\n",
        "        for i, (input_ids, masks, labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), position=0, leave=True):\n",
        "            input_ids = input_ids.cuda()\n",
        "            masks = masks.cuda()\n",
        "            labels = labels.cuda()              \n",
        "            optimizer.zero_grad() \n",
        "            \n",
        "            output = network(input_ids, masks)\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "            train_epoch_acc += torch.eq(pred, labels).sum().item()\n",
        "            loss = loss_fn(output,labels)/batch_size\n",
        "\n",
        "            # commented lines are codes for BertSeqeucen\n",
        "            #output = network(input_ids, masks, labels=labels)       \n",
        "            #pred = torch.argmax(output.logits, dim=1)\n",
        "            #loss = output.loss\n",
        "            train_epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc[epoch] = train_epoch_acc / len(train_dataset)\n",
        "        train_loss[epoch] = train_epoch_loss / len(train_dataloader)\n",
        "        print(f\"train_loss:{train_loss[epoch]:.6f}, train_acc:{train_acc[epoch]:.6f}\")\n",
        "        if (train_acc[epoch] > train_best_acc):\n",
        "          train_best_acc = train_acc[epoch]\n",
        "          torch.save(network.state_dict(), f\"{directory}/snapshot_train_best\")\n",
        "\n",
        "        scheduler.step(test_acc[epoch//val_freq])\n",
        "\n",
        "    network.eval()   \n",
        "    test_loss[-1], test_acc[-1] = evaluate(network, val_dataloader, loss_fn, len(test_dataset))    \n",
        "    torch.save(train_loss, f\"{directory}/train_loss\")\n",
        "    torch.save(test_loss, f\"{directory}/test_loss\")\n",
        "    torch.save(train_acc, f\"{directory}/train_acc\")\n",
        "    torch.save(test_acc, f\"{directory}/test_acc\")\n",
        "    torch.save(network.state_dict(), f\"{directory}/snapshot_final\")\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA9kqy7dKfuG"
      },
      "source": [
        "def get_path(loss, opt, lr, batch_size, epoch, freeze):\n",
        "    return f\"{PATH}/lr_{lr}_bs_{batch_size}_epoch_{epoch}_freeze_{freeze}\""
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIyrsQftKh0w"
      },
      "source": [
        "loss = 'ce'\n",
        "opt = 'adam'\n",
        "freeze = 0\n",
        "batch_size = 32\n",
        "lr = 1e-5\n",
        "num_epochs = 12"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G85wXkN7KjB8"
      },
      "source": [
        "PATH = \"bert_bilstm_classifier\"\n",
        "directory = f\"{get_path(loss, opt, lr, batch_size, num_epochs, freeze)}\"\n",
        "os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(11747)\n",
        "num_layers = 2"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tqUcRLkKkrx"
      },
      "source": [
        "network = BertBiLSTM(num_layers, num_classes, freeze=freeze)\n",
        "network = network.cuda()"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RviaF6bLjxG"
      },
      "source": [
        "optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=2, verbose=True, mode='max')"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "3-MX7YxyLwvB",
        "outputId": "bc963c1c-e795-44fe-d426-ca8a81096c9f"
      },
      "source": [
        "# Trained for Unfreezed BERT\n",
        "train(directory, network, loss_fn, train_dataset, val_dataset, optimizer, scheduler, batch_size, num_epochs)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 335/335 [00:13<00:00, 24.60it/s]\n",
            "  0%|          | 1/3022 [00:00<08:48,  5.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  0, test_loss: 0.092450, test_acc: 0.00037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3022/3022 [06:47<00:00,  7.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss:0.032031, train_acc:0.752782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3022/3022 [06:47<00:00,  7.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss:0.014977, train_acc:0.878419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 335/335 [00:13<00:00, 24.41it/s]\n",
            "  0%|          | 1/3022 [00:00<06:59,  7.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  2, test_loss: 0.013435, test_acc: 0.88422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3022/3022 [06:48<00:00,  7.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss:0.012783, train_acc:0.888318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3022/3022 [06:47<00:00,  7.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss:0.011365, train_acc:0.897213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 335/335 [00:13<00:00, 24.30it/s]\n",
            "  0%|          | 1/3022 [00:00<06:48,  7.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  4, test_loss: 0.012667, test_acc: 0.88076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3022/3022 [06:46<00:00,  7.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss:0.010117, train_acc:0.905975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3022/3022 [06:48<00:00,  7.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_loss:0.008990, train_acc:0.915056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 3/335 [00:00<00:13, 24.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch     6: reducing learning rate of group 0 to 7.0000e-06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 335/335 [00:13<00:00, 24.49it/s]\n",
            "  0%|          | 1/3022 [00:00<07:30,  6.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  6, test_loss: 0.013946, test_acc: 0.87356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 47/3022 [00:06<06:47,  7.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-812933d49024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Trained for Unfreezed BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-119-53f3ab033941>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(directory, network, loss_fn, train_dataset, test_dataset, optimizer, scheduler, batch_size, num_epochs, verbose, val_freq)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m                     \u001b[0;31m# If no `miniters` was specified, adjust automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1435\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# request flush on the background thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# wait for flush to actually get through, if we can.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# waiting across threads during import can cause deadlocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 )\n\u001b[1;32m    504\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVx8jDifcJfH"
      },
      "source": [
        "Get the tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYXc9V4Zb0DO"
      },
      "source": [
        "network = BertBiLSTM(num_layers, num_classes)\n",
        "network.load_state_dict(torch.load(f\"{directory}/snapshot_val_best\"))\n",
        "network.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDAcSOpbcLYB"
      },
      "source": [
        "def inference(network, dataset, labels, batch_size):\n",
        "  logits = []\n",
        "  preds = []\n",
        "  loader = DataLoader(dataset, shuffle=False, batch_size=batch_size)\n",
        "  acc = 0.0\n",
        "  network.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (input_ids, masks, labels) in tqdm(enumerate(loader),total=len(loader),position=0, leave=True):\n",
        "      input_ids = input_ids.cuda()\n",
        "      masks = masks.cuda()\n",
        "      labels = labels.cuda()\n",
        "            \n",
        "      output = network(input_ids, masks)\n",
        "      logits.append(output)\n",
        "      pred = torch.argmax(output,dim=1)\n",
        "      preds.extend([label2index[str(pred[i].item())] for i in range(len(pred))])\n",
        "      #output = network(input_ids, masks, labels=labels)\n",
        "      #preds = torch.argmax(output.logits, dim=1)\n",
        "      #loss = output.loss\n",
        "      acc += torch.eq(pred, labels).sum().item()\n",
        "    \n",
        "  return torch.cat(logits), preds, acc / len(dataset)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmiBSbu4fA2H",
        "outputId": "93df05d7-455b-47b5-9954-9767e779df4e"
      },
      "source": [
        "val_logits, val_preds, val_acc = inference(network, val_dataset, val_labels, 32)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 335/335 [00:13<00:00, 24.37it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVa2V3wAH5bg",
        "outputId": "b2784b2e-7b29-498e-d309-7616a21f0260"
      },
      "source": [
        "print(val_acc)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8842233236696905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HkHBiBiH6yw",
        "outputId": "c9ac0691-01df-4eff-c5ee-c5e2d957512a"
      },
      "source": [
        "print(val_preds[:10])"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PERSON', 'CARDINAL', 'LOC', 'DATE', 'CARDINAL', 'PERSON', 'PERSON', 'DATE', 'PERSON', 'OTHERS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmcYDxsdjxBB",
        "outputId": "976baea6-4b40-4ec0-d246-1d8f72c4f54b"
      },
      "source": [
        "tr_logits, tr_preds, tr_acc = inference(network, train_dataset, tr_labels, 32)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3022/3022 [02:04<00:00, 24.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiBS6tmEK6o_",
        "outputId": "97e62299-b987-4359-d5d7-26ddbde2ad21"
      },
      "source": [
        "print(tr_acc)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8874281103893418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4zE1-NfhN_C"
      },
      "source": [
        "torch.save(val_logits, f\"{directory}/val_logits\")"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w48X-NDPhVfp"
      },
      "source": [
        "torch.save(tr_logits, f\"{directory}/tr_logits\")"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFJGCRqXlJvl"
      },
      "source": [
        "with open(f\"{directory}/val_preds.tsv\", 'w') as v_f:\n",
        "  v_f.write('\\n'.join(val_preds))"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcrYnqojliP2"
      },
      "source": [
        "with open(f\"{directory}/tr_preds.tsv\", 'w') as t_f:\n",
        "  t_f.write('\\n'.join(tr_preds))"
      ],
      "execution_count": 155,
      "outputs": []
    }
  ]
}