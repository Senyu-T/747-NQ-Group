{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_answer_type.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYYi8QrbvE83"
      },
      "source": [
        "!git clone https://github.com/Senyu-T/unifiedqa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp3IW0BkvNVy"
      },
      "source": [
        "cd unifiedqa/bart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F002YTgovQ-B"
      },
      "source": [
        "!chmod +x download_data.sh; ./download_data.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xunOi5R_vTba",
        "outputId": "eecc55c3-27c6-4484-c95b-d022ee71c4aa"
      },
      "source": [
        "cd data/natural_questions_with_dpr_para/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/unifiedqa/bart/data/natural_questions_with_dpr_para\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYvSyqfOEQrp"
      },
      "source": [
        "Normalize answers by removing special tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqhcSnpF0HoQ"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def normalize_answer(s):\n",
        "  def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def remove_spc_token(s):\n",
        "  s = s.replace(' \\\\\\'\\\\\\'', ' \\'\\'')   # double quotation\n",
        "  s = s.replace('\\\\\\'', '\\'')\n",
        "  s = s.replace(' \\'s', '\\'s')    # 's\n",
        "  s = s.replace(' ,', ',')\n",
        "  return s"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MzLcgWuvwqx"
      },
      "source": [
        "# read file, parse into context / question / answer for further data analysis\n",
        "def read_files(file_name):\n",
        "  answers = []\n",
        "  questions = []\n",
        "  contexts = []\n",
        "  with open(file_name, 'rb') as inference_in:\n",
        "    lines = inference_in.readlines()\n",
        "    for i in range(len(lines)):\n",
        "      sep = str(lines[i]).split('\\\\n') \n",
        "      questions.append(sep[0][2:-1])\n",
        "      ans = (sep[1].split('\\\\t')[-1]).lower()\n",
        "      ans = normalize_answer(remove_spc_token(ans))  # normalize answers\n",
        "      answers.append(ans)\n",
        "      contexts.append(sep[1].split('\\\\t')[0])\n",
        "  return answers, questions, contexts"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6B8Fk1UEZGj"
      },
      "source": [
        "Get Spacy NER model running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkeZhv65iU5K"
      },
      "source": [
        "!pip3 install spacy-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVFZsEOGh7qw"
      },
      "source": [
        "!python3 -m spacy download en_core_web_trf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9giDoM_J5th"
      },
      "source": [
        "import spacy\n",
        "spacy.require_gpu() \n",
        "sp_lg = spacy.load('en_core_web_trf')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F24rCqhaLzpa"
      },
      "source": [
        "def spacy_large_ner(document, ans):\n",
        "  #print({(ent.text.strip(), ent.label_) for ent in sp_lg(document).ents})\n",
        "  for ent in sp_lg(document).ents:\n",
        "    if ans == ent.text.strip():\n",
        "      return ent.label_"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOsx7iP6EmiN"
      },
      "source": [
        "Retrieve the sentence that contains the answer from the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kuwfhqLeo0e"
      },
      "source": [
        "import time"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhujqCB0EkfM"
      },
      "source": [
        "def get_tags(answer_list, q_list, context_list):\n",
        "  soft_tags = []\n",
        "  hard_tags = []\n",
        "  start_time = time.time()\n",
        "  for i in range(len(answer_list)):\n",
        "    q_tokens = q_list[i].split(' ')\n",
        "    start = q_tokens[0].replace('\\\\', '')\n",
        "    # we use NER to get the answer type, in case NER produce\n",
        "    # \"None\" for obvious types, we craft the tag according to\n",
        "    # the first token\n",
        "    docs = context_list[i].split('- -') # divide by wiki entries\n",
        "    ans = answer_list[i]\n",
        "    ans_type = None\n",
        "    # run NER based on retrieved document contexts\n",
        "    for doc in docs:\n",
        "      if doc.find(ans):\n",
        "        ans_type = spacy_large_ner(doc, ans)\n",
        "        if ans_type is not None:\n",
        "          break\n",
        "\n",
        "    # in case NER model fails to find type based on context\n",
        "    # find answer type based on the answer itself\n",
        "    if ans_type is None:\n",
        "      ans_type = spacy_large_ner(ans, ans)\n",
        "    \n",
        "\n",
        "    hard_ans_type = None\n",
        "    if start == \"who\": # ignores ORG, just use \n",
        "      hard_ans_type = 'PERSON'\n",
        "    elif start == \"what\" and q_tokens[1] == \"number\":\n",
        "      hard_ans_type = 'CARDINAL'\n",
        "    elif start == \"how\" and q_tokens[1] == \"many\":\n",
        "      hard_ans_type = 'CARDINAL'\n",
        "    elif start == \"when\":\n",
        "      hard_ans_type = \"DATE\"\n",
        "    elif start == \"where\":\n",
        "      hard_ans_type = \"LOC\"\n",
        "    else:\n",
        "      hard_ans_type = \"OTHERS\"\n",
        "\n",
        "    # if NER fails again, we use hard type as soft type\n",
        "    if ans_type is None:\n",
        "      ans_type = hard_ans_type\n",
        "    # append anser type\n",
        "    soft_tags.append(ans_type)\n",
        "    hard_tags.append(hard_ans_type)\n",
        "    if i % 100 == 0:\n",
        "      print(f\"lines {i:5d} out of {len(answer_list)} done in {time.time() - start_time:.2f} seconds\")\n",
        "  assert(len(soft_tags) == len(answer_list))\n",
        "  assert(len(hard_tags) == len(answer_list))\n",
        "  return hard_tags, soft_tags"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-KzFhvQX_Mz"
      },
      "source": [
        "Now we conduct the experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttf8lFvP05Qp"
      },
      "source": [
        "dev_answers, dev_questions, dev_contexts = read_files(\"dev.tsv\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCxmiMXBYBOq",
        "outputId": "2821a51b-7889-4f57-e8bd-1c1229652dce"
      },
      "source": [
        "sample_a = dev_answers[40:51]\n",
        "sample_c = dev_contexts[40:51]\n",
        "sample_q = dev_questions[40:51]\n",
        "\n",
        "sample_ht, sample_st = get_tags(sample_a,sample_q,sample_c)\n",
        "print(sample_ht)\n",
        "print(sample_st)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines     0 out of 11 done in 0.04 seconds\n",
            "['PERSON', 'PERSON', 'OTHERS', 'OTHERS', 'PERSON', 'OTHERS', 'OTHERS', 'PERSON', 'PERSON', 'PERSON', 'PERSON']\n",
            "['PERSON', 'GPE', 'ORG', 'PERSON', 'PERSON', 'OTHERS', 'OTHERS', 'PERSON', 'PERSON', 'PERSON', 'PERSON']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9bMk3xYYojn",
        "outputId": "0fc0ee49-8b7a-4f6c-dfa1-9373aa74af18"
      },
      "source": [
        "print(sample_a)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['andy griffith', 'brazil', 'walmart', 'mcdonalds', 'cyndi lauper', 'ymir fritz', 'artax', 'alexander graham bell', 'drake', 'cece', 'blair']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2Y_n1-1Y2GO",
        "outputId": "f4cc1129-2c01-43bd-ca42-6a155cbe14f0"
      },
      "source": [
        "print(sample_q)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['who is the old man in waiting on a woman?', 'who have won the world cup the most times?', 'list of companies with highest number of employees?', 'list of companies with highest number of employees?', 'who sang the original version of true colors?', 'in attack on titan who is the female titan?', 'what is the horses name in never-ending story?', 'who made the first telephone in the world?', 'who is charles off of pretty little liars?', 'who is charles off of pretty little liars?', 'who got pregnant in gossip girl season 5?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEVevcPGbWTE"
      },
      "source": [
        "Now we get the writer works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbTEW2FpbVm3"
      },
      "source": [
        "def write_res(tag_list, output_f):\n",
        "  with open(output_f, 'wt') as f:\n",
        "    f.write('\\n'.join(tag_list))\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JosLyAbidhM8"
      },
      "source": [
        "dev_ht, dev_st = get_tags(dev_answers, dev_questions, dev_contexts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73l1ZMwYqDU1"
      },
      "source": [
        "write_res(dev_st, \"dev_soft_tag.tsv\")\n",
        "write_res(dev_ht, \"dev_hard_tag.tsv\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1i6JhXnqVxO"
      },
      "source": [
        "train_answers, train_questions, train_contexts = read_files(\"train.tsv\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9nFqmIRqRFF",
        "outputId": "f3350202-af1a-42e6-def9-15855b7d7e19"
      },
      "source": [
        "train_ht, train_st = get_tags(train_answers, train_questions, train_contexts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines     0 out of 96676 done in 0.15 seconds\n",
            "lines   100 out of 96676 done in 10.34 seconds\n",
            "lines   200 out of 96676 done in 20.13 seconds\n",
            "lines   300 out of 96676 done in 29.70 seconds\n",
            "lines   400 out of 96676 done in 39.23 seconds\n",
            "lines   500 out of 96676 done in 49.75 seconds\n",
            "lines   600 out of 96676 done in 60.67 seconds\n",
            "lines   700 out of 96676 done in 71.36 seconds\n",
            "lines   800 out of 96676 done in 81.82 seconds\n",
            "lines   900 out of 96676 done in 93.37 seconds\n",
            "lines  1000 out of 96676 done in 104.09 seconds\n",
            "lines  1100 out of 96676 done in 114.48 seconds\n",
            "lines  1200 out of 96676 done in 125.19 seconds\n",
            "lines  1300 out of 96676 done in 135.84 seconds\n",
            "lines  1400 out of 96676 done in 146.38 seconds\n",
            "lines  1500 out of 96676 done in 157.38 seconds\n",
            "lines  1600 out of 96676 done in 167.12 seconds\n",
            "lines  1700 out of 96676 done in 177.11 seconds\n",
            "lines  1800 out of 96676 done in 187.38 seconds\n",
            "lines  1900 out of 96676 done in 197.87 seconds\n",
            "lines  2000 out of 96676 done in 207.60 seconds\n",
            "lines  2100 out of 96676 done in 217.66 seconds\n",
            "lines  2200 out of 96676 done in 228.01 seconds\n",
            "lines  2300 out of 96676 done in 238.64 seconds\n",
            "lines  2400 out of 96676 done in 248.10 seconds\n",
            "lines  2500 out of 96676 done in 258.08 seconds\n",
            "lines  2600 out of 96676 done in 269.08 seconds\n",
            "lines  2700 out of 96676 done in 278.09 seconds\n",
            "lines  2800 out of 96676 done in 288.07 seconds\n",
            "lines  2900 out of 96676 done in 297.51 seconds\n",
            "lines  3000 out of 96676 done in 307.27 seconds\n",
            "lines  3100 out of 96676 done in 316.36 seconds\n",
            "lines  3200 out of 96676 done in 326.51 seconds\n",
            "lines  3300 out of 96676 done in 335.42 seconds\n",
            "lines  3400 out of 96676 done in 346.10 seconds\n",
            "lines  3500 out of 96676 done in 355.41 seconds\n",
            "lines  3600 out of 96676 done in 364.55 seconds\n",
            "lines  3700 out of 96676 done in 373.55 seconds\n",
            "lines  3800 out of 96676 done in 384.36 seconds\n",
            "lines  3900 out of 96676 done in 395.38 seconds\n",
            "lines  4000 out of 96676 done in 405.41 seconds\n",
            "lines  4100 out of 96676 done in 415.95 seconds\n",
            "lines  4200 out of 96676 done in 424.60 seconds\n",
            "lines  4300 out of 96676 done in 434.75 seconds\n",
            "lines  4400 out of 96676 done in 444.96 seconds\n",
            "lines  4500 out of 96676 done in 455.14 seconds\n",
            "lines  4600 out of 96676 done in 465.88 seconds\n",
            "lines  4700 out of 96676 done in 476.03 seconds\n",
            "lines  4800 out of 96676 done in 486.06 seconds\n",
            "lines  4900 out of 96676 done in 495.81 seconds\n",
            "lines  5000 out of 96676 done in 506.35 seconds\n",
            "lines  5100 out of 96676 done in 515.11 seconds\n",
            "lines  5200 out of 96676 done in 524.59 seconds\n",
            "lines  5300 out of 96676 done in 535.05 seconds\n",
            "lines  5400 out of 96676 done in 545.09 seconds\n",
            "lines  5500 out of 96676 done in 554.90 seconds\n",
            "lines  5600 out of 96676 done in 565.49 seconds\n",
            "lines  5700 out of 96676 done in 576.61 seconds\n",
            "lines  5800 out of 96676 done in 586.35 seconds\n",
            "lines  5900 out of 96676 done in 595.03 seconds\n",
            "lines  6000 out of 96676 done in 605.10 seconds\n",
            "lines  6100 out of 96676 done in 616.50 seconds\n",
            "lines  6200 out of 96676 done in 626.91 seconds\n",
            "lines  6300 out of 96676 done in 636.34 seconds\n",
            "lines  6400 out of 96676 done in 645.92 seconds\n",
            "lines  6500 out of 96676 done in 656.41 seconds\n",
            "lines  6600 out of 96676 done in 666.09 seconds\n",
            "lines  6700 out of 96676 done in 674.62 seconds\n",
            "lines  6800 out of 96676 done in 684.64 seconds\n",
            "lines  6900 out of 96676 done in 694.81 seconds\n",
            "lines  7000 out of 96676 done in 704.38 seconds\n",
            "lines  7100 out of 96676 done in 714.78 seconds\n",
            "lines  7200 out of 96676 done in 723.80 seconds\n",
            "lines  7300 out of 96676 done in 732.78 seconds\n",
            "lines  7400 out of 96676 done in 743.78 seconds\n",
            "lines  7500 out of 96676 done in 752.73 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}